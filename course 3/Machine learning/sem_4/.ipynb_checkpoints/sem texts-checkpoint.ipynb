{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоп слова, лемматизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymorphy2 as pm2\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "#--------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lemmatizer and stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "russian_stopwords = stopwords.words(\"russian\") + ['onca', 'хищный']\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('jaguar.csv')\n",
    "pmm = pm2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление стоп слов и лемматизация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = [pmm.normal_forms(x)[0] for x in text.split() if x not in russian_stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def tokenize_ru(file_text):\n",
    "#    tokens = word_tokenize(file_text)\n",
    "#    #tokens = [i for i in tokens if (i not in string.punctuation)]\n",
    "#    stop_words = stopwords.words('russian') + ['onca']\n",
    "#    tokens = [i for i in tokens if (i not in stop_words)]\n",
    "#    tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "#    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очищенный текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean = sent_tokenize(data['body'], 'russian')\n",
    "#clean = clean.apply(parse)\n",
    "clean = data['body'].apply(parse)\n",
    "abstracts = [[word for word in clean if word not in russian_stopwords] for c in clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean\n",
    "\n",
    "#model = Word2Vec(abstracts[0], size=300, window=5, min_count=5, iter=10)\n",
    "#model.init_sims(replace = True)\n",
    "#model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(abstracts[0], size=300, window=5, min_count=5, iter=10)\n",
    "model.init_sims(replace = True)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter()\n",
    "for abstract in abstracts[0]:\n",
    "    cnt.update(abstract)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terms = [i[0] for i in cnt.most_common(5000)]\n",
    "terms = [i[0] for i in cnt.most_common(5000) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_vec = model[terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "terms_tsne = tsne.fit_transform(terms_vec)\n",
    "#plot TSNE\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "source = ColumnDataSource(data=dict(x1=terms_tsne[:,0],\n",
    "                                    x2=terms_tsne[:,1],\n",
    "                                    labels = terms))\n",
    "\n",
    "hover = HoverTool(tooltips = [(\"label\",\"@labels\")])\n",
    "\n",
    "p = figure(title=\"TSNE for terms\", tools = [hover])\n",
    "\n",
    "\n",
    "\n",
    "p.scatter(x=\"x1\", y=\"x2\", size=3, source=source)\n",
    "\n",
    "from scipy.cluster.hierarchy import  ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) \n",
    "fig, ax = pyplot.subplots(figsize=(10, 250)) \n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=terms);\n",
    "\n",
    "pyplot.tick_params(\\\n",
    "    axis= 'x',          \n",
    "    which='both',      \n",
    "    bottom='off',   \n",
    "    top='off',      \n",
    "    labelbottom='off')\n",
    "\n",
    "pyplot.tight_layout() \n",
    "\n",
    "pyplot.savefig('w2v_clusters.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "terms_tsne = tsne.fit_transform(terms_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [' '.join(a) for a in abstracts[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.append(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.append(np.array(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = KMeans(n_clusters=2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = np.array(cl.fit(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import  ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(kk) \n",
    "fig, ax = pyplot.subplots(figsize=(10, 250)) \n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=terms);\n",
    "\n",
    "pyplot.tick_params(\\\n",
    "    axis= 'x',          \n",
    "    which='both',      \n",
    "    bottom='off',   \n",
    "    top='off',      \n",
    "    labelbottom='off')\n",
    "\n",
    "pyplot.tight_layout() \n",
    "\n",
    "pyplot.savefig('w2v_clusters.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
