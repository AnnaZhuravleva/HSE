In clusters of galaxies, the specific entropy of intracluster plasma increases outwards. Nevertheless, a number of recent studies have shown that the intracluster medium is subject to buoyancy instabilities due to the effects of cosmic rays and anisotropic thermal conduction. In this paper, we present a new numerical algorithm for simulating such instabilities. This numerical method treats the cosmic rays as a fluid, accounts for the diffusion of heat and cosmic rays along magnetic field lines, and enforces the condition that the temperature and cosmic-ray pressure remain positive. We carry out several tests to ensure the accuracy of the code, including the detailed matching of analytic results for the eigenfunctions and growth rates of linear buoyancy instabilities. This numerical scheme will be useful for simulating convection driven by cosmic-ray buoyancy in galaxy cluster plasmas and may also be useful for other applications, including fusion plasmas, the interstellar medium, and supernovae remnants.