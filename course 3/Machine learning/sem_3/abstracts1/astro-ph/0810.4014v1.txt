We address the question of whether numerical particle-in-cell (PIC) simulations and laboratory laser-plasma experiments can (or will be able to, in the near future) model realistic gamma-ray burst (GRB) shocks. For this, we compare the radiative cooling time, t_cool, of relativistic electrons in the shock magnetic fields to the microscopic dynamical time of collisionless relativistic shocks -- the inverse plasma frequency of protons, omega_pp^{-1}. We obtain that for t_cool*omega_pp^{-1}\lesssim ~few hundred, the electrons cool efficiently at or near the shock jump and are capable of emitiing away a large fraction of the shock energy. Such shocks are well-resolved in existing PIC simulations; therefore, the microscopic structure can be studied in detail. Since most of the emission in such shocks would be coming from the vicinity of the shock, the spectral power of the emitted radiation can be directly obtained from finite-length simulations and compared with observational data. Such radiative shocks correspond to the internal baryon-dominated GRB shocks for the conventional range of ejecta parameters. Fermi acceleration of electrons in such shocks is limited by electron cooling, hence the emitted spectrum should be lacking a non-thermal tail, whereas its peak likely falls in the multi-MeV range. Incidentally, the conditions in internal shocks are almost identical to those in laser-produced plasmas; thus, such GRB-like plasmas can be created and studied in laboratory experiments using the presently available Petawatt-scale laser facilities. An analysis of the external shocks shows that only the highly relativistic shocks, corresponding to the extremely early afterglow phase, can have efficient electron cooling in the shock transition. We emphasize the importance of radiative PIC simulations for further studies.