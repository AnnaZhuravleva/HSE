Observational cosmology provides us with a large number of high precision data which are used to derive models trying to reproduce ``on the mean'' our observable patch of the Universe. Most of these attempts are achieved in the framework of a Friedmann-Lema\^itre cosmology where large scale homogeneity is assumed. However, we know, from the observation of structures at increasing scales, that these models are only approximations of a smoothed or averaged inhomogeneous underlying patern. Anyhow, when modelling the Universe, the usual method is to use continuous functions representing the kinematical scalars of the velocity field, implicitly assuming that they represent volume averages of the corresponding fine-scale inhomogeneous quantities, then put them into the Einstein equations which are solved to give the model and its dependance upon a number of parameters arbitrarily defined. In General Relativity, such a method is very much involved since the equations which determine the metric tensor and the quantities calculated from it are highly nonlinear. The question raised by the method consisting of determining the parameters of an a priori assumed FLRW model from observational data is the ``fitting problem'' brought to general attention by Ellis and Stoeger in the 80's. This problem has recently experienced a reniewed attention due to the amount of available data and the increase of the minimum scale at which homogeneity can be assumed. We propose a discussion of this issue in the light of the latest developments of observational and theoretical cosmology.