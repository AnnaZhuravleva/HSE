We study the long term evolution of magnetic fields generated by a collisionless relativistic $e^+e^-$ shock which is initially unmagnetized. Our 2D particle-in-cell numerical simulations show that downstream of such a Weibel-mediated shock, particle distributions are close to isotropic, relativistic Maxwellians, and the magnetic turbulence is highly intermittent spatially, with the non-propagating magnetic fields forming relatively isolated regions with transverse dimension $\sim 10-20$ skin depths. These structures decay in amplitude, with little sign of downstream merging. The fields start with magnetic energy density $\sim (0.1-0.2)$ of the upstream kinetic energy within the shock transition, but rapid downstream decay drives the fields to much smaller values, below $10^{-3}$ of equipartition after $10^3$ skin depths.   In an attempt to construct a theory that follows field decay to these smaller values, we explore the hypothesis that the observed damping is a variant of Landau damping in an unmagnetized plasma. The model is based on the small value of the downstream magnetic energy density, which suggests that particle orbits are only weakly perturbed from straight line motion, if the turbulence is homogeneous. Using linear kinetic theory applied to electromagnetic fields in an isotropic, relativistic Maxwellian plasma, we find a simple analytic form for the damping rates, $\gamma_k$, in two and three dimensions for small amplitude, subluminous electromagnetic fields. We find that magnetic energy does damp due to phase mixing of current carrying particles as $(\omega_p t)^{-q}$ with $q \sim 1$. (abridged)