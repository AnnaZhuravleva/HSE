Spectroscopic modeling of Type II supernovae (SNe) generally assumes steady-state. Following the recent suggestion of Utrobin & Chugai, but using the 1D non-LTE line-blanketed model atmosphere code CMFGEN, we investigate the effects of including time-dependent terms that appear in the statistical and radiative equilibrium equations. We base our discussion on the ejecta properties and the spectroscopic signatures obtained from time-dependent simulations, investigating different ejecta configurations, and covering their evolution from one day to six weeks after shock breakout. Compared to equivalent steady-state models, our time-dependent models produce SN ejecta that are systematically over-ionized, affecting helium at one week after explosion, but ultimately affecting all ions after a few weeks. While the continuum remains essentially unchanged, time-dependence effects on observed spectral lines are large. At the recombination epoch, HI lines and NaID are considerably stronger and broader than in equivalent steady-state models, while CaII8500A is weakened. If time dependence is allowed for, the HeI lines at 5875A and 10830A appear about 3 times stronger at one week, and HeI10830A persists as a blue-shifted absorption feature even at 6 weeks after explosion. Time dependence operates through the energy gain from changes in ionization and excitation, and, perhaps more universally across SN types, from the competition between recombination and expansion, which in-turn, can be affected by optical-depth effects. Our time-dependent models compare well with observations of the low-luminosity low-velocity SN 1999br and the more standard SN 1999em, reproducing the Halpha line strength at the recombination epoch, and without the need for setting unphysical requirements on the magnitude of nickel mixing.