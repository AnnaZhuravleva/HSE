Galactic cosmic rays are widely believed to be accelerated in expanding shock waves initiated by supernova explosions. The theory of diffusive shock acceleration of cosmic rays is now well established, but two fundamental questions remain partly unanswered: what is the acceleration efficiency, i.e. the fraction of the total supernova energy converted to cosmic-ray energy, and what is the maximum kinetic energy achieved by particles accelerated in supernova explosions? Recent observations of supernova remnants, in X-rays with the Chandra and XMM-Newton satellites and in very-high-energy gamma rays with several ground-based atmospheric Cerenkov telescopes, have provided new pieces of information concerning these two questions. After a review of these observations and their current interpretations, I show that complementary information on the diffusive shock acceleration process can be obtained by studying the radio emission from extragalactic supernovae. As an illustration, a nonlinear model of diffusive shock acceleration is applied to the radio light curves of the supernova SN 1993J, which exploded in the nearby galaxy M81. The results of the model suggest that most of the Galactic cosmic rays may be accelerated during the early phase of interaction between the supernova ejecta and the wind lost from the progenitor star.