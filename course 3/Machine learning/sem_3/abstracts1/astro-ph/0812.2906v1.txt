We have tested the two main theoretical models of bubbles around massive star clusters, Castor et al. and Chevalier & Clegg, against observations of the well studied Carina Nebula. The Castor et al. theory over-predicts the X-ray luminosity in the Carina bubble by a factor of 60 and expands too rapidly, by a factor of 4; if the correct radius and age are used, the predicted X-ray luminosity is even larger. In contrast, the Chevalier & Clegg model under-predicts the X-ray luminosity by a factor of 10. We modify the Castor et al. theory to take into account lower stellar wind mass loss rates, radiation pressure, gravity, and escape of or energy loss from the hot shocked gas. We argue that energy is advected rather than radiated from the bubble. We undertake a parameter study for reduced stellar mass loss rates and for various leakage rates and are able to find viable models. The X-ray surface brightness in Carina is highest close to the bubble wall, which is consistent with conductive evaporation from cold clouds. The picture that emerges is one in which the hot gas pressure is far below that found by dividing the time-integrated wind luminosity by the bubble volume; rather, the pressure in the hot gas is set by pressure equilibrium with the photoionized gas at T=10^4 K. It follows that the shocked stellar winds are not dynamically important in forming the bubbles.