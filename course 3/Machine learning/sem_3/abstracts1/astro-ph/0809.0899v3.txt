Based on the intensity and rates of various kinds of intense ionizing radiation events such as supernovae and gamma-ray bursts, it is likely that the Earth has been subjected to one or extinction level events during the Phanerozoic. These induce changes in atmospheric chemistry so that the level of Solar ultraviolet-B radiation reaching the surface and near-surface waters may be doubled for up to a decade. This UVB level is known from experiment to be more than enough to kill off many kinds of organisms, particularly phytoplankton. It could easily induce a crash of the photosynthetic-based food chain in the oceans. Regularities in the latitudinal distribution of damage are apparent in simulations of the atmospheric changes. We previously proposed that the late Ordovician extinction is a plausible candidate for a contribution from an ionizing radiation event, based on environmental selectivity in trilobites. To test a null hypothesis based on this proposal, we confront latitudinal differential extinction rates predicted from the simulations with data from a published analysis of latitudinal gradients in the Ordovician extinction. The pattern of UVB damage always shows a strong maximum at some latitude, with substantially lower intensity to the north and south of this maximum. We find that the pattern of damage predicted from our simulations is consistent with the data assuming a burst approximately over the South Pole, and no further north than -75 degrees. We predict that any land mass (such as parts of north China, Laurentia, and New Guinea) which then lay north of the equator should be a refuge from UVB effects, and show a different pattern of extinction in the first strike of the end-Ordovician extinction, if induced by such a radiation event.