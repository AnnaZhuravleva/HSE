The dust extinction of gamma-ray bursts (GRBs) host galaxies, containing important clues to the nature of GRB progenitors and crucial for dereddening, is still poorly known. Here we propose a straightforward method to determine the extinction of GRB host galaxies by comparing the observed optical spectra to the intrinsic ones extrapolated from the X-ray spectra. The rationale for this method is from the standard fireball model: if the optical flux decay index equals to that of the X-ray flux, then there is no break frequency between the optical and X-ray bands, therefore we can derive the intrinsic optical flux from the X-ray spectra. We apply this method to three GRBs of which the optical and X-ray fluxes have the same decay indices and another one with inferred cooling break frequency, and obtain the rest-frame extinction curves of their host galaxies. The derived extinction curves are gray and do not resemble any extinction curves of local galaxies (e.g. the Milk Way, the Small/Large Magellanic Clouds, or nearby starburst galaxies). The amount of extinction is rather large (with visual extinction $A_V$ $\sim$ 1.6--3.4$\magni$). We model the derived extinction curves in terms of the silicate-graphite interstellar grain model. As expected from the ``gray'' nature of the derived extinction curve, the dust size distribution is skewed to large grains. We determine, for the first time, the local dust-to-gas ratios of GRB host galaxies using the model-derived dust parameters and the hydrogen column densities determined from X-ray absorptions.