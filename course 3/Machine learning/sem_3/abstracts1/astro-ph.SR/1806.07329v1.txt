We assess total-variation methods to denoise gravitational-wave signals in real noise conditions, by injecting numerical-relativity waveforms from core-collapse supernovae and binary black hole mergers in data from the first observing run of Advanced LIGO. This work is an extension of our previous investigation where only Gaussian noise was used. Since the quality of the results depends on the regularization parameter of the model, we perform an heuristic search for the value that produces the best results. We discuss various approaches for the selection of this parameter, either based on the optimal, mean, or multiple values, and compare the results of the denoising upon these choices. Moreover, we also present a machine-learning-informed approach to obtain the Lagrange multiplier of the method through an automatic search. Our results provide further evidence that total-variation methods can be useful in the field of Gravitational-Wave Astronomy as a tool to remove noise.