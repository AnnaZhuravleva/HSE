Distance measures between quantum states like the trace distance and the fidelity can naturally be defined by optimizing a classical distance measure over all measurement statistics that can be obtained from the respective quantum states. In contrast, Petz showed that the measured relative entropy, defined as a maximization of the Kullback-Leibler divergence over projective measurement statistics, is strictly smaller than Umegaki's quantum relative entropy whenever the states do not commute. We extend this result in two ways. First, we show that Petz' conclusion remains true if we allow general positive operator valued measures. Second, we extend the result to Renyi relative entropies and show that for non-commuting states the sandwiched Renyi relative entropy is strictly larger than the measured Renyi relative entropy for $\alpha \in (\frac12, \infty)$, and strictly smaller for $\alpha \in [0,\frac12)$. The latter statement provides counterexamples for the data-processing inequality of the sandwiched Renyi relative entropy for $\alpha < \frac12$. Our main tool is a new variational expression for the measured Renyi relative entropy, which we further exploit to show that certain lower bounds on quantum conditional mutual information are superadditive.