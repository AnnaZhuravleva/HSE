Randomized benchmarking (RB) is widely used to measure an error rate of a set of quantum gates, by performing random circuits that would do nothing if the gates were perfect. In the limit of no finite-sampling error, the exponential decay rate of the observable survival probabilities, versus circuit length, yields a single error metric $r$. For Clifford gates with arbitrary small errors described by process matrices, $r$ was believed to reliably correspond to the mean, over all Cliffords, of the average gate infidelity (AGI) between the imperfect gates and their ideal counterparts. We show that this quantity is not a well-defined property of a physical gateset. It depends on the representations used for the imperfect and ideal gates, and the variant typically computed in the literature can differ from $r$ by orders of magnitude. We present new theories of the RB decay that are accurate for all small errors describable by process matrices, and show that the RB decay curve is a simple exponential for all such errors. These theories allow explicit computation of the error rate that RB measures ($r$), but as far as we can tell it does not correspond to the infidelity of a physically allowed (completely positive) representation of the imperfect gates.