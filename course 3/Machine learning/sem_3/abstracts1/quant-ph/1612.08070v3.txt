Understanding computational speed-up is fundamental for the development of efficient quantum algorithms. In this paper, we study such problem under the framework of the Quantum Query Model, which represents the probability of output $x \in \{0,1\}^n$ as a function $\pi(x)$, and denotes by $L(\pi)$ the spectral norm for $\pi$ defined over the Fourier basis of the linear space of functions $f: \left\{0,1\right\}^{n} \rightarrow \mathbb{R}$. We then present a classical simulation for output probabilities $\pi$, whose error depends on $L(\pi)$. Such dependence implies upper-bounds for the quotient between the number of queries of an optimal classical algorithm and our quantum algorithm, respectively. These upper-bounds show a strong relation between spectral norm and quantum parallelism. This result also implies that there is no asymptotic quantum speed-up for a sequence of boolean functions of constant degree.