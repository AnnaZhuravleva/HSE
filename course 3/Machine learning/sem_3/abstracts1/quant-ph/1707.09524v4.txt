Ridge regression (RR) is an important machine learning technique which introduces a regularization hyperparameter to ordinary multiple linear regression for analyzing data suffering from multicollinearity. In this paper, we present a quantum algorithm for RR, where by giving the technique of parallel Hamiltonian simulation that can simulate a number of Hermitian matrices in parallel, we develop a quantum version of $K$-fold cross-validation approach that can efficiently estimate the predictive performance of RR. Our algorithm consists of two phases: (1) using quantum $K$-fold cross-validation to efficiently determine a good regularization hyperparameter for RR with which RR can achieve good predictive performance, then (2) generating a quantum state encoding the optimal fitting parameters of RR with such hyperparameter, which can be further utilized to predict new data. Since efficient simulation of indefinite density Hamiltonians \cite{RSL} is adopted as the key subroutine, our algorithm is able to handle non-sparse data matrices. It is shown that our algorithm can achieve exponential speedup over the classical counterpart for (low-rank) data matrices with low condition numbers. But when the condition numbers of data matrices is large to be amenable to full or approximately full ranks of data matrices, polynomial speedup can be achieved.