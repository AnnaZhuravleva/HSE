Photon losses are the strongest imperfection affecting boson sampling experiments. Despite their importance, little is known about the resilience of boson sampling to losses. In this work we show that all current architectures that suffer from an exponential decay of the transmission with the depth of the circuit, such as integrated photonic circuits or optical fibers, can be efficiently simulated classically. We prove that either the depth of the circuit is large enough that it can be simulated by thermal noise with an algorithm running in polynomial time, or it is short enough that a tensor network simulation runs in quasi-polynomial time. This result suggest that in order to implement a quantum advantage experiment with single-photons and linear-optics we need a profound change of paradigm.