{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Recap\n",
    "\n",
    "## Outline\n",
    "1. Linear Algebra\n",
    "1. Probability Theory\n",
    "1. Information Theory\n",
    "1. Numerical Optimization\n",
    "\n",
    "## Readings\n",
    "\n",
    "1. GoodFellow. Deep Learning. p43-106\n",
    "1. recap.pdf\n",
    "1. (Positive Definite Matrix) http://mlwiki.org/index.php/Positive-Definite_Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Linear Algebra\n",
    "\n",
    "**Vector Space**\n",
    "1. $0$: $x + 0 = x$\n",
    "1. $(-x)$: $x + (-x) = 0$\n",
    "1. $1$: $1x = x$\n",
    "1. $x + y = y + x$\n",
    "1. $(x + y) + z = x + (y + z)$ \n",
    "1. $\\alpha (x+y) = \\alpha x + \\alpha y$ and $(\\alpha + \\beta) x = \\alpha x + \\beta x$\n",
    "<img src=\"images/vector.png\" style=\"height:200px\">\n",
    "\n",
    "**Scalar Product**\n",
    "$ < *| * >$ : $ V x V \\rightarrow R$\n",
    "1. $<x|x> \\geq 0$\n",
    "1. $<0|0> = 0 $\n",
    "1. $<x|y> = <y|x>$\n",
    "1. $<x+y|z> = <x|z> + <y|z>$ and $<\\alpha x| y> = \\alpha <x|y>$\n",
    "<img src=\"images/scalar.png\" style=\"height:200px\">\n",
    "\n",
    "**Norm**  \n",
    "1. $||x|| \\geq 0$\n",
    "2. $||0|| = 0$\n",
    "3. $||\\alpha x || = |\\alpha | ||x||$\n",
    "4. $|| x + y || \\leq ||x|| + ||y||$\n",
    "\n",
    "**Metric**  \n",
    "1. $d(x,y) \\geq 0$\n",
    "2. $d(x,x) = 0$\n",
    "3. $d(x,y) = d(y,x)$\n",
    "4. $d(x,y) \\leq d(x,z) + d(y,z)$\n",
    "<img src=\"images/metric.svg\" style=\"height:200px\">\n",
    "\n",
    "**Linear Mapping**  \n",
    "$L: V \\rightarrow W$\n",
    "1. $L(\\alpha x + \\beta y) = \\alpha L(x) + \\beta L(y)$\n",
    "\n",
    "**Linear Operator**  \n",
    "$L: V \\rightarrow V$\n",
    "1. $L(\\alpha x + \\beta y) = \\alpha L(x) + \\beta L(y)$\n",
    "\n",
    "**Inverse Matrix**  \n",
    "$\\exists! B = A^{-1}$ : $AB = BA = I$ iff $det(A) > 0$\n",
    "\n",
    "**Orthogonal Matrix**  \n",
    "$QQ^T = Q^TQ = I$\n",
    "\n",
    "**Posititve definite matrix**  \n",
    "for $\\forall x$:  $x^T A x \\geq 0$\n",
    "\n",
    "**Singular Value Decomposition**  \n",
    "$A = U \\Sigma V^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Probability Theory\n",
    "**Probability**  \n",
    "$$p(x)$$\n",
    "\n",
    "**Conditional Probability**    \n",
    "<img src=\"images/cond.png\" style=\"height:200px\">\n",
    "\n",
    "**Joint Probability**     \n",
    "$$p(x,y) = p(x | y) p(y) = p(y|x)p(x)$$\n",
    "\n",
    "**Bayes Theorem**  \n",
    "<img src=\"images/bayes.jpg\" style=\"height:300px\">\n",
    "\n",
    "**Expected Mean**  \n",
    "$$E[x] = \\int x p(x) dx$$\n",
    "\n",
    "**Variance**  \n",
    "$$Var[x] = \\int (E[x] - x)^2 p(x) dx$$\n",
    "\n",
    "**Covariance**  \n",
    "$$Var[x_i, x_j] = \\int (E[x_i] - x_i)(E[x_j] - x_j) p(x_i, x_j) dx_i dx_j$$\n",
    "\n",
    "**Correlation**  \n",
    "$$\\rho(x_i, x_j) = \\frac {Var[x_i, x_j]} {Var[x_i] Var[x_j]}$$\n",
    "\n",
    "**Normal Probability Distribution**  \n",
    "$$N(m, \\sigma) = \\frac 1 {\\sqrt{2\\pi} \\sigma} \\exp^{-\\frac {(x - m)^2} {2\\sigma^2}}$$\n",
    "<img src=\"images/normal.png\" style=\"height:200px\">\n",
    "\n",
    "**Maximum Likelihood Estimation**  \n",
    "i.i.d. samples ${x_1, .. , x_n}$  \n",
    "Likelihood function:   \n",
    "$$L(\\theta) = p(x_1, ..., x_n; \\theta) = \\prod_i p(x_i; \\theta)$$  \n",
    "$$\\hat \\theta_{MLE} = \\arg\\max_{\\theta} \\log L(\\theta) = \\arg\\max_{\\theta} \\sum_i \\log p(x_i; \\theta) $$  \n",
    "\n",
    "**Maximum a posteriori estimation**   \n",
    "i.i.d. samples ${x_1, .. , x_n}$ \n",
    "\n",
    "posterior probability:  \n",
    "$$p(\\theta | x_1, ...,x_n) = \\frac {p(\\theta) p(x_1, ..., x_n | \\theta)} {p(x_1, ..., x_n)}$$  \n",
    "$$\\hat \\theta_{MAP} = \\arg\\max_{\\theta} p(\\theta) p(x_1, ..., x_n | \\theta) = \\arg\\max (\\log p(\\theta) + \\sum_i \\log p(x_i; \\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Information Theory\n",
    "\n",
    "**Entropy**  \n",
    "$$ H(p) = - \\sum_i p_i \\log p_i$$ for discrete  \n",
    "$$ H(p) = - \\int p(x) \\log p(x) dx $$ for continious  \n",
    "\n",
    "<img src=\"images/entropy.png\" style=\"height:200px\">\n",
    "\n",
    "**Cross-entropy**\n",
    "\n",
    "$$ H(p,q) = - \\sum_i p_i \\log q_i$$ for discrete\n",
    "$$ H(p,q) = - \\int p(x) \\log q(x) dx$$ for continious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Numerical Optimization  \n",
    "$$f: R^n \\rightarrow R$$  \n",
    "$$f(x) = f(x_1, x_2, ... x_n)$$  \n",
    "\n",
    "**Derivative**\n",
    "<img src=\"images/derivative.svg\" style=\"height:200px\">\n",
    "\n",
    "**Gradient**  \n",
    "$$[\\nabla f]_i = \\frac {\\partial f} {\\partial x_i}$$\n",
    "<img src=\"images/grad.png\" style=\"height:200px\">\n",
    "\n",
    "**Hessian**  \n",
    "$$[\\nabla^2 f]_{i,j} = \\frac {\\partial^2 f} {\\partial x_i \\partial x_j}$$\n",
    "\n",
    "**Gradient Decent**\n",
    "\n",
    "$$f(x) \\rightarrow \\min_{x}$$\n",
    "\n",
    "$$ x_i^{(t)} \\leftarrow x_i^{(t-1)} - \\alpha [\\nabla f(x^{(t-1)}) ]_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
